Data Analysis Summary

            #another way of reading csv file beside pandas
            #import csv
            #f = open('C:/Users/jessiel/Desktop/Myself/Kaggle/label_categories.csv')
            #csv_f = csv.reader(f)
            #for row in csv_f:
            #  print row

import pandas as pd
1. event
df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/events.csv')
#print df
print len(df.event_id)
print len(df.event_id.unique())

Result:
3252950
3252950

print len(df.device_id)
print len(df.device_id.unique())

Result:
3252950
60865

Data Preprocesing:
Not necessary

2. app_event

df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/app_events.csv')
#print df
print len(df.app_id)
print len(df.app_id.unique())

Result: 
32473067
19237

print len(df.event_id)
print len(df.event_id.unique())

Result:
32473067
1488096

3. app_labels
df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/app_labels.csv')
#print df.head(10)
print len(df.app_id)
print len(df.app_id.unique())

Result:
459943
113211

Data Preprocessing:
df_uniq_app = df.drop_duplicates()
print len(df_uniq_app.app_id)
print len(df_uniq_app.app_id.unique())

Result:
459452
113211

Solution: remove only one duplicated row, since the app_id in this table is not primary key
(app_id and label_id can be used as composite PF), no need to do further clean up.

df_uniq_app.to_csv('app_labels_cleaned.csv')
df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/app_labels_cleaned.csv')
print len(df.app_id)

Result:
459452

4. label_categories
df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/label_categories.csv')
#print df.head(10)
print len(df.label_id)
print len(df.label_id.unique())

Result:
930
930

Data Preprocessing: Remove lines which category = Null/Unknown
df_notNull = df[df.category.notnull()]
df_notNull_removeUnknown = df_notNull.loc[df_notNull.category != 'unknown']

Result:
print len(df_notNull) -------927
print len(df_notNull_removeUnknown) -------901
print len(df)  -----930


5. phone_brand_device_model
df =pd.read_csv('C:/Users/jessiel/Desktop/Myself/Kaggle/phone_brand_device_model.csv')
#print df.head(10)
print len(df.device_id)
print len(df.device_id.unique())

Result:
187245
186716

Data Preprocessing:
df_uniq_model = df.drop_duplicates()
print len(df_uniq_model.device_id)
print len(df_uniq_model.device_id.unique())

Result:
186722
186716

Solution: remove (186722- 186716)*2 = 12 lines of duplicated rows and save it as phone_brand_device_model_cleaned.csv file

#print df_uniq_model[(df_uniq_model.groupby('device_id').transform('count')>1).values].drop_duplicates() 
#print len(df_uniq_model[(df_uniq_model.groupby('device_id').transform('count')>1).values].drop_duplicates())
#the above sentence is used to find the 6 duplicated rows, totally 12 rows

df_final = df_uniq_model.drop_duplicates(subset = 'device_id',keep = False)
print len(df_final.device_id)
print len(df_final.device_id.unique())
df_final.to_csv('phone_brand_device_model_cleaned.csv')

Result: 
186710
186710



Appendix: Some other source code
(df_uniq_model = df.drop_duplicates()
#print df.drop_duplicates()
#print len(df.device_id)
#print len(df.device_id.unique())\
#print df_uniq_model.device_id.duplicated()

#df_dup_id = df_uniq_model.groupby(df_uniq_model.device_id).size()
df_dup_id = df_uniq_model.groupby(['device_id']).size())
